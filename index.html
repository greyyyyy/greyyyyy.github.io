<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/23/RNN%E5%AE%9E%E7%8E%B0%E8%BF%9E%E7%BB%AD%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/23/RNN%E5%AE%9E%E7%8E%B0%E8%BF%9E%E7%BB%AD%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B/" class="post-title-link" itemprop="url">RNN实现连续数据预测</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-02-23 19:26:44" itemprop="dateCreated datePublished" datetime="2021-02-23T19:26:44+08:00">2021-02-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-25 14:13:05" itemprop="dateModified" datetime="2021-02-25T14:13:05+08:00">2021-02-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>RNN，也就是循环神经网络，实现连续数据预测。   </p>
<h1 id="回顾卷积神经网络"><a href="#回顾卷积神经网络" class="headerlink" title="回顾卷积神经网络"></a>回顾卷积神经网络</h1><p>卷积就是特征提取器，就是<code>CBAPD</code>————C(卷积),B(BN，批标准化),A(激活),P(池化),D(舍弃)<br>卷积神经网络，就是借助卷积核提取空间特征后，送入全连接网络。<br>而有些数据是和空间序列相关的，能够通过连续性进行预测。   </p>
<h1 id="循环核"><a href="#循环核" class="headerlink" title="循环核"></a>循环核</h1><p><strong>定义：*</strong> 参与时间共享，循环层提取时间信息<br>循环核具有记忆力，通过不同时刻的参数共享，实现了对时间序列的信息提取<br><img src="https://i.loli.net/2021/02/23/GPWMR94uZwxF3fH.png" alt="image.png"><br>👆这是记忆体，可以设定记忆体的个数，改变记忆容量，当记忆体个数被指定，输入Xt，Yt维度被指定，周围的待训练参数量维度也就被限定了。<br>记忆体内存储着每个时刻的状态信息ht，记忆体当前时刻存储的状态信息<code>ht=tanh(Xt*Wxh+h(t-1)*Whh+bh)</code>,当前时刻，循环核的输出特征<code>yt=softmax(ht*Why+by)</code>,实际上，得到输出的yt的这个过程就是一层全连接。<br>三个参数<code>Wxh,Whh,Why</code>,正如<code>w,b</code>一样，在前向传播时，是固定不变的；在反向传播时，被梯度下降法更新。   </p>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>循环核按时间步展开，也就是把循环核按照时间轴方向展开。<br><img src="https://i.loli.net/2021/02/23/BNpfJ8HXDwA53me.png" alt="image.png"><br>每个时刻，记忆体状态信息ht被刷新，记忆体周围的参数<code>Wxh,Whh,Why</code>是不变的，训练优化的对象就是这些参数矩阵<br>训练完成后，选用效果最好的参数矩阵，执行前向传播，输出预测结果。<br>循环神经网络就是借助循环核提取时间特征后，送入全连接网络，实现连续数据的预测。<br><img src="https://i.loli.net/2021/02/23/MfmTOu9v2GDgU34.png" alt="image.png"></p>
<h1 id="循环计算层"><a href="#循环计算层" class="headerlink" title="循环计算层"></a>循环计算层</h1><p>每个循环核构成一层循环计算层。<br>循环计算层的层数是着向输出方向生长的。<br><img src="https://i.loli.net/2021/02/23/GBPtUSpKAOcvnTo.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/23/9HuhvPI86GAXeEo.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/23/kEi2ZB1odtI4WUz.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/23/FE1rMDqseKpLch7.png" alt="image.png"><br>入RNN时，是有维度要求的，x的维度要是三维的<code>(送入样本数,循环核时间展开步数,每个时间步输入特征个数)</code><br><img src="https://i.loli.net/2021/02/23/ma2zvbZpFnsOUf1.png" alt="image.png">    </p>
<h1 id="示例：字母预测"><a href="#示例：字母预测" class="headerlink" title="示例：字母预测"></a>示例：字母预测</h1><p><strong>目标：</strong>输入一个字母，预测出下一个字母；<br>首先，神经网络预测的直接结果都是数字，所以我们要先把要用到的五个字母用数字表示出来，我们用<code>one-hot</code>的方法来表示。<br><img src="https://i.loli.net/2021/02/23/FxyNuJk62W1P8Lr.png" alt="image.png"><br>随机生成三个参数矩阵<br><img src="https://i.loli.net/2021/02/23/WAi2qxGPrMuLyTU.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/23/lJIna3oXAgrxK5C.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/23/iOfZ8b9HsWt7Gq5.png" alt="image.png"><br>当前输入字母b，也就是<code>[0,1,0,0,0]</code>，乘上<code>Wxh</code>,再加上上一个状态<code>ht*Whh</code>,再加上偏置项<br><img src="https://i.loli.net/2021/02/23/Ko8ETwgOPvRtHsI.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/23/fOMgnlj8bxGurLI.png" alt="image.png">  </p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, SimpleRNN
import matplotlib.pyplot as plt
import os

input_word = &quot;abcde&quot;
w_to_id = {&apos;a&apos;: 0, &apos;b&apos;: 1, &apos;c&apos;: 2, &apos;d&apos;: 3, &apos;e&apos;: 4}  # 单词映射到数值id的词典
id_to_onehot = {0: [1., 0., 0., 0., 0.], 1: [0., 1., 0., 0., 0.], 2: [0., 0., 1., 0., 0.], 3: [0., 0., 0., 1., 0.],
                4: [0., 0., 0., 0., 1.]}  # id编码为one-hot

x_train = [id_to_onehot[w_to_id[&apos;a&apos;]], id_to_onehot[w_to_id[&apos;b&apos;]], id_to_onehot[w_to_id[&apos;c&apos;]],
        id_to_onehot[w_to_id[&apos;d&apos;]], id_to_onehot[w_to_id[&apos;e&apos;]]]
y_train = [w_to_id[&apos;b&apos;], w_to_id[&apos;c&apos;], w_to_id[&apos;d&apos;], w_to_id[&apos;e&apos;], w_to_id[&apos;a&apos;]]

np.random.seed(7)
np.random.shuffle(x_train)
np.random.seed(7)
np.random.shuffle(y_train)
tf.random.set_seed(7)

# 使x_train符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。
# 此处整个数据集送入，送入样本数为len(x_train)；输入1个字母出结果，循环核时间展开步数为1; 表示为独热码有5个输入特征，每个时间步输入特征个数为5
x_train = np.reshape(x_train, (len(x_train), 1, 5))
y_train = np.array(y_train)

model = tf.keras.Sequential([
    SimpleRNN(3),
    Dense(5, activation=&apos;softmax&apos;)
])

model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=[&apos;sparse_categorical_accuracy&apos;])

checkpoint_save_path = &quot;./checkpoint/rnn_onehot_1pre1.ckpt&quot;

if os.path.exists(checkpoint_save_path + &apos;.index&apos;):
    print(&apos;-------------load the model-----------------&apos;)
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                save_weights_only=True,
                                                save_best_only=True,
                                                monitor=&apos;loss&apos;)  # 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型

history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback])

model.summary()

# print(model.trainable_variables)
file = open(&apos;./weights.txt&apos;, &apos;w&apos;)  # 参数提取
for v in model.trainable_variables:
    file.write(str(v.name) + &apos;\n&apos;)
    file.write(str(v.shape) + &apos;\n&apos;)
    file.write(str(v.numpy()) + &apos;\n&apos;)
file.close()

###############################################    show   ###############################################

# 显示训练集和验证集的acc和loss曲线
acc = history.history[&apos;sparse_categorical_accuracy&apos;]
loss = history.history[&apos;loss&apos;]

plt.subplot(1, 2, 1)
plt.plot(acc, label=&apos;Training Accuracy&apos;)
plt.title(&apos;Training Accuracy&apos;)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label=&apos;Training Loss&apos;)
plt.title(&apos;Training Loss&apos;)
plt.legend()
plt.show()

############### predict #############

preNum = int(input(&quot;input the number of test alphabet:&quot;))
for i in range(preNum):
    alphabet1 = input(&quot;input test alphabet:&quot;)
    alphabet = [id_to_onehot[w_to_id[alphabet1]]]
    # 使alphabet符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。此处验证效果送入了1个样本，送入样本数为1；输入1个字母出结果，所以循环核时间展开步数为1; 表示为独热码有5个输入特征，每个时间步输入特征个数为5
    alphabet = np.reshape(alphabet, (1, 1, 5))
    result = model.predict([alphabet])
    pred = tf.argmax(result, axis=1)
    pred = int(pred)
    tf.print(alphabet1 + &apos;-&gt;&apos; + input_word[pred])</code></pre><h1 id="示例深化"><a href="#示例深化" class="headerlink" title="示例深化"></a>示例深化</h1><p><strong>目的:</strong>连续输入四个字母预测下一个字母。<br><img src="https://i.loli.net/2021/02/24/DfkZJi9K1cMEFvm.png" alt="image.png"><br>讲解循环核按时间展开后，循环计算过程<br>使用三个记忆体<br><img src="https://i.loli.net/2021/02/24/AyKJpg2fh8BEHQz.png" alt="image.png"><br>初始时刻，记忆体内的记忆是0<br><img src="https://i.loli.net/2021/02/24/rvMx9lQ3dtHmhfo.png" alt="image.png">      </p>
<h1 id="Embedding编码"><a href="#Embedding编码" class="headerlink" title="Embedding编码"></a>Embedding编码</h1><p><strong>Embedding：</strong>是一种单词编码方法，向低维向量实现了编码。这种编码通过神经网络训练优化，能表达出单词间的相关性。   </p>
<pre><code>✨tf.keras.layers.Embedding(词汇表大小,编码维度)   
# 编码维度就是用几个数字表达一个单词   
# 对1-100的数字进行编码，且用三个数字来表达一个数   
# tf.keras.layers.Embedding(100,3)   

# 入Embedding，要求x的维度是二维   
✨[送入样本数,循环核时间展开步数]    </code></pre><hr>
<p><strong>下面进行刚刚的例子用<code>Embedding</code>编码复现</strong>     </p>
<pre><code># 入Embedding要[送入样本数,循环核时间展开步数]  
# 入RNN要[样本数,循环核时间展开步数,样本特征数]   

# 1.import   

import numpy as np  
import tensorflow as tf
from tensorflow.keras.layers import Dense,SimpleRNN,Embedding   
import matplotlib.pyplot as plt   

word = &apos;abcde&apos;  
w_to_id = {&apos;a&apos;:0;&apos;b&apos;:1;&apos;c&apos;:2;&apos;d&apos;:3;&apos;e&apos;:4}

# 2. train、test 
# 输入一个字母对应输出下一个字母，也就是下一个字母是上一个字母的标签   
x_train =[w_to_id[&apos;a&apos;],w_to_id[&apos;b&apos;],w_to_id[&apos;c&apos;],w_to_id[&apos;d&apos;],w_to_id[&apos;e&apos;]]
y_train =[w_to_id[&apos;b&apos;],w_to_id[&apos;c&apos;],w_to_id[&apos;d&apos;],w_to_id[&apos;e&apos;],w_to_id[&apos;a&apos;]]  

# 打乱顺序    
np.random.seed(7)
np.random.shuffle(x_train)   
np.random.seed(7)   
np.random.shuffle(y_train)   
tf.random.set_seed(7)    

# 准备对应维度   
x_train.reshape(x_train,(len(x_train),1))   
y_train = np.array(y_train)    

# 4.sequential   
model = tf.keras.Sequential([
    Embedding(5,2) #这一层会生成一个五行两列的可训练参数矩阵
    SimpleRNN(3)  #设定了具有三个记忆体的循环层
    Dense(5,activation=&apos;softmax&apos;)
])   

# 5.compile   
model.compile(optimizer=tf.keras.optimizer.Adam(0.01),loss=tf.keras.losses.SparseCategoricalCrossentroy(from_logits=False),metrics=[&apos;sparse_categorical_accuracy&apos;])   

checkpoint_save_path = &quot;./checkpoint/run_embedding_1pre1.ckpt&quot;

if os.path.exists(checkpoint_save_path + &apos;.index&apos;):
    print(&apos;-------------load the model-----------------&apos;)
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                save_weights_only=True,
                                                save_best_only=True,
                                                monitor=&apos;loss&apos;)  # 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型

history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback])

model.summary()

# print(model.trainable_variables)
file = open(&apos;./weights.txt&apos;, &apos;w&apos;)  # 参数提取
for v in model.trainable_variables:
    file.write(str(v.name) + &apos;\n&apos;)
    file.write(str(v.shape) + &apos;\n&apos;)
    file.write(str(v.numpy()) + &apos;\n&apos;)
file.close()

###############################################    show   ###############################################

# 显示训练集和验证集的acc和loss曲线
acc = history.history[&apos;sparse_categorical_accuracy&apos;]
loss = history.history[&apos;loss&apos;]

plt.subplot(1, 2, 1)
plt.plot(acc, label=&apos;Training Accuracy&apos;)
plt.title(&apos;Training Accuracy&apos;)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label=&apos;Training Loss&apos;)
plt.title(&apos;Training Loss&apos;)
plt.legend()
plt.show()

############### predict #############

preNum = int(input(&quot;input the number of test alphabet:&quot;))
for i in range(preNum):
    alphabet1 = input(&quot;input test alphabet:&quot;)
    alphabet = [w_to_id[alphabet1]]
    # 使alphabet符合Embedding输入要求：[送入样本数， 循环核时间展开步数]。
    # 此处验证效果送入了1个样本，送入样本数为1；输入1个字母出结果，循环核时间展开步数为1。
    alphabet = np.reshape(alphabet, (1, 1))
    result = model.predict(alphabet)
    pred = tf.argmax(result, axis=1)
    pred = int(pred)
    tf.print(alphabet1 + &apos;-&gt;&apos; + input_word[pred])</code></pre><hr>
<p><strong>下面深化复现</strong><br><img src="https://i.loli.net/2021/02/24/1BZ2DPwjtLvx5Cb.png" alt="image.png">   </p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, SimpleRNN, Embedding
import matplotlib.pyplot as plt
import os

input_word = &quot;abcdefghijklmnopqrstuvwxyz&quot;
w_to_id = {&apos;a&apos;: 0, &apos;b&apos;: 1, &apos;c&apos;: 2, &apos;d&apos;: 3, &apos;e&apos;: 4,
        &apos;f&apos;: 5, &apos;g&apos;: 6, &apos;h&apos;: 7, &apos;i&apos;: 8, &apos;j&apos;: 9,
        &apos;k&apos;: 10, &apos;l&apos;: 11, &apos;m&apos;: 12, &apos;n&apos;: 13, &apos;o&apos;: 14,
        &apos;p&apos;: 15, &apos;q&apos;: 16, &apos;r&apos;: 17, &apos;s&apos;: 18, &apos;t&apos;: 19,
        &apos;u&apos;: 20, &apos;v&apos;: 21, &apos;w&apos;: 22, &apos;x&apos;: 23, &apos;y&apos;: 24, &apos;z&apos;: 25}  # 单词映射到数值id的词典

training_set_scaled = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
                    11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
                    21, 22, 23, 24, 25]

x_train = []
y_train = []

for i in range(4, 26):
    x_train.append(training_set_scaled[i - 4:i])
    y_train.append(training_set_scaled[i])

np.random.seed(7)
np.random.shuffle(x_train)
np.random.seed(7)
np.random.shuffle(y_train)
tf.random.set_seed(7)

# 使x_train符合Embedding输入要求：[送入样本数， 循环核时间展开步数] ，
# 此处整个数据集送入所以送入，送入样本数为len(x_train)；输入4个字母出结果，循环核时间展开步数为4。
x_train = np.reshape(x_train, (len(x_train), 4))
y_train = np.array(y_train)

model = tf.keras.Sequential([
    Embedding(26, 2),
    SimpleRNN(10),
    Dense(26, activation=&apos;softmax&apos;)
])

model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=[&apos;sparse_categorical_accuracy&apos;])

checkpoint_save_path = &quot;./checkpoint/rnn_embedding_4pre1.ckpt&quot;

if os.path.exists(checkpoint_save_path + &apos;.index&apos;):
    print(&apos;-------------load the model-----------------&apos;)
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                save_weights_only=True,
                                                save_best_only=True,
                                                monitor=&apos;loss&apos;)  # 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型

history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback])

model.summary()

file = open(&apos;./weights.txt&apos;, &apos;w&apos;)  # 参数提取
for v in model.trainable_variables:
    file.write(str(v.name) + &apos;\n&apos;)
    file.write(str(v.shape) + &apos;\n&apos;)
    file.write(str(v.numpy()) + &apos;\n&apos;)
file.close()

###############################################    show   ###############################################

# 显示训练集和验证集的acc和loss曲线
acc = history.history[&apos;sparse_categorical_accuracy&apos;]
loss = history.history[&apos;loss&apos;]

plt.subplot(1, 2, 1)
plt.plot(acc, label=&apos;Training Accuracy&apos;)
plt.title(&apos;Training Accuracy&apos;)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label=&apos;Training Loss&apos;)
plt.title(&apos;Training Loss&apos;)
plt.legend()
plt.show()

################# predict ##################

preNum = int(input(&quot;input the number of test alphabet:&quot;))
for i in range(preNum):
    alphabet1 = input(&quot;input test alphabet:&quot;)
    alphabet = [w_to_id[a] for a in alphabet1]
    # 使alphabet符合Embedding输入要求：[送入样本数， 时间展开步数]。
    # 此处验证效果送入了1个样本，送入样本数为1；输入4个字母出结果，循环核时间展开步数为4。
    alphabet = np.reshape(alphabet, (1, 4))
    result = model.predict([alphabet])
    pred = tf.argmax(result, axis=1)
    pred = int(pred)
    tf.print(alphabet1 + &apos;-&gt;&apos; + input_word[pred])</code></pre><h1 id="RNN实现股票预测"><a href="#RNN实现股票预测" class="headerlink" title="RNN实现股票预测"></a>RNN实现股票预测</h1><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p><img src="https://i.loli.net/2021/02/24/SzeoTcbw3GRMO6P.png" alt="image.png"><br>这个文件是用tushare模块下载的SH600519贵州茅台日k线数据<br>在该例中，只用C列的，也就是开盘价的，连续60天的数据，预测第61天的开盘价    </p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dropout, Dense, SimpleRNN
import matplotlib.pyplot as plt
import os
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math

maotai = pd.read_csv(&apos;./SH600519.csv&apos;)  # 读取股票文件

training_set = maotai.iloc[0:2426 - 300, 2:3].values  # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价
test_set = maotai.iloc[2426 - 300:, 2:3].values  # 后300天的开盘价作为测试集

# 归一化
sc = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间
training_set_scaled = sc.fit_transform(training_set)  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化
test_set = sc.transform(test_set)  # 利用训练集的属性对测试集进行归一化

x_train = []
y_train = []

x_test = []
y_test = []

# 测试集：csv表格中前2426-300=2126天数据
# 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。
for i in range(60, len(training_set_scaled)):
    x_train.append(training_set_scaled[i - 60:i, 0])
    y_train.append(training_set_scaled[i, 0])
# 对训练集进行打乱
np.random.seed(7)
np.random.shuffle(x_train)
np.random.seed(7)
np.random.shuffle(y_train)
tf.random.set_seed(7)
# 将训练集由list格式变为array格式
x_train, y_train = np.array(x_train), np.array(y_train)

# 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。
# 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1
x_train = np.reshape(x_train, (x_train.shape[0], 60, 1))
# 测试集：csv表格中后300天数据
# 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。
for i in range(60, len(test_set)):
    x_test.append(test_set[i - 60:i, 0])
    y_test.append(test_set[i, 0])
# 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]
x_test, y_test = np.array(x_test), np.array(y_test)
x_test = np.reshape(x_test, (x_test.shape[0], 60, 1))

model = tf.keras.Sequential([
    SimpleRNN(80, return_sequences=True),
    Dropout(0.2),
    SimpleRNN(100),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
            loss=&apos;mean_squared_error&apos;)  # 损失函数用均方误差
# 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值

checkpoint_save_path = &quot;./checkpoint/rnn_stock.ckpt&quot;

if os.path.exists(checkpoint_save_path + &apos;.index&apos;):
    print(&apos;-------------load the model-----------------&apos;)
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                save_weights_only=True,
                                                save_best_only=True,
                                                monitor=&apos;val_loss&apos;)

history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1,
                    callbacks=[cp_callback])

model.summary()

file = open(&apos;./weights.txt&apos;, &apos;w&apos;)  # 参数提取
for v in model.trainable_variables:
    file.write(str(v.name) + &apos;\n&apos;)
    file.write(str(v.shape) + &apos;\n&apos;)
    file.write(str(v.numpy()) + &apos;\n&apos;)
file.close()

loss = history.history[&apos;loss&apos;]
val_loss = history.history[&apos;val_loss&apos;]

plt.plot(loss, label=&apos;Training Loss&apos;)
plt.plot(val_loss, label=&apos;Validation Loss&apos;)
plt.title(&apos;Training and Validation Loss&apos;)
plt.legend()
plt.show()

################## predict ######################
# 测试集输入模型进行预测
predicted_stock_price = model.predict(x_test)
# 对预测数据还原---从（0，1）反归一化到原始范围
predicted_stock_price = sc.inverse_transform(predicted_stock_price)
# 对真实数据还原---从（0，1）反归一化到原始范围
real_stock_price = sc.inverse_transform(test_set[60:])
# 画出真实数据和预测数据的对比曲线
plt.plot(real_stock_price, color=&apos;red&apos;, label=&apos;MaoTai Stock Price&apos;)
plt.plot(predicted_stock_price, color=&apos;blue&apos;, label=&apos;Predicted MaoTai Stock Price&apos;)
plt.title(&apos;MaoTai Stock Price Prediction&apos;)
plt.xlabel(&apos;Time&apos;)
plt.ylabel(&apos;MaoTai Stock Price&apos;)
plt.legend()
plt.show()

##########evaluate##############
# calculate MSE 均方误差 ---&gt; E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)
mse = mean_squared_error(predicted_stock_price, real_stock_price)
# calculate RMSE 均方根误差---&gt;sqrt[MSE]    (对均方误差开方)
rmse = math.sqrt(mean_squared_error(predicted_stock_price, real_stock_price))
# calculate MAE 平均绝对误差-----&gt;E[|预测值-真实值|](预测值减真实值求绝对值后求均值）
mae = mean_absolute_error(predicted_stock_price, real_stock_price)
print(&apos;均方误差: %.6f&apos; % mse)
print(&apos;均方根误差: %.6f&apos; % rmse)
print(&apos;平均绝对误差: %.6f&apos; % mae)</code></pre><p><img src="https://i.loli.net/2021/02/24/VZBmNuoQhYUbIF5.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/24/Bv5hmYe4XfAMDPd.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/24/T2SuIe3LNvCF1mf.png" alt="image.png"></p>
<h1 id="用LSTM实现股票预测"><a href="#用LSTM实现股票预测" class="headerlink" title="用LSTM实现股票预测"></a>用LSTM实现股票预测</h1><h2 id="LSTM介绍"><a href="#LSTM介绍" class="headerlink" title="LSTM介绍"></a>LSTM介绍</h2><p>传统循环网络RNN可以通过记忆体实现短期记忆，进行连续数据的预测，但当连续数据的序列变长时，会使展开时间步过长，在反向传播更新参数时，梯度要按照时间步连续相乘，会导致梯度消失。<br>所以提出了长短记忆网络LSTM<br>LSTM中引入了三个门限<br><img src="https://i.loli.net/2021/02/24/ZEYtFkfPIaJHcnB.png" alt="image.png"><br>三个门限，都是当前时刻输入特征xt和上个时刻的短期记忆h(t-1)的函数。<code>Wi，Wf，Wo</code>都是待训练参数矩阵,<code>bi,bf,bo</code>都是待训练偏置项，最后经过<code>sigmoid</code>激活函数，使门限的范围在0-1之间<br>细胞态是<code>上个时刻的长期记忆*遗忘门+当前时刻归纳出的新知识*输入门</code><br>记忆体，短期记忆，是长期记忆的一部分。是细胞态过激活函数<code>tanh</code>，乘上输出门的结果<br>候选态表示归纳出的待存入细胞态的新知识<br><img src="https://i.loli.net/2021/02/24/wXM2PFarYDnygVt.png" alt="image.png">   </p>
<p>LSTM就是我们听课的过程。如果我们听课是由ppt授课。那么我们脑袋里现在记住的内容就是我们的长期记忆，是当前页ppt到第一张ppt整个长期记忆。<br>长期记忆由两部分组成，一部分是起始页到前一页的知识乘上遗忘门，也就是前面的内容在当前剩余的部分，也就是过去的记忆。另一部分是当前页的新知识，是即将存入脑中的现在的记忆。<br>现在的记忆：由当前时刻的输入<code>xt</code>和上一页的短期记忆留存<code>ht</code>归纳形成的新知识乘上输入门。<br>现在的记忆和过去的记忆一同存储为长期记忆。<br>当进行复述时，不可能一字不落的讲出来，我们讲的是留存在脑中的长期记忆经过输出门筛选后的内容。这就是记忆体的输出ht<br>当有多层循环网络时，第二层的循环网络的输入xt就是第一层循环网络的输出ht。输入第二层网络的是第一层网络提取出来的精华。   </p>
<h2 id="LSTM的Tensorflow实现"><a href="#LSTM的Tensorflow实现" class="headerlink" title="LSTM的Tensorflow实现"></a>LSTM的Tensorflow实现</h2><p><strong>tf.keras.layers.LSTM(记忆体个数,return_sequences=是否返回输出)</strong>   </p>
<ul>
<li>return_sequences 是否每个时间步都输出ht   <ul>
<li>True 每个时间输出ht  </li>
<li>False 仅在最后时间步输出ht(默认)   </li>
</ul>
</li>
</ul>
<p><strong>🎱举例</strong>   </p>
<pre><code>model = tf.keras.Sequential([
    LSTM(80,return_sequences=True),
    Dropout(0.2)
    LSTM(100)
    Dropout(0.2)
    Dense(1)
])   </code></pre><h2 id="LSTM实现股票预测"><a href="#LSTM实现股票预测" class="headerlink" title="LSTM实现股票预测"></a>LSTM实现股票预测</h2><p>只需改动   </p>
<ul>
<li>import部分，不再import SimpleRNN    </li>
<li>model部分改成上面的样子即可    </li>
</ul>
<h1 id="GRU实现股票预测"><a href="#GRU实现股票预测" class="headerlink" title="GRU实现股票预测"></a>GRU实现股票预测</h1><h2 id="GRU介绍"><a href="#GRU介绍" class="headerlink" title="GRU介绍"></a>GRU介绍</h2><p>GRU使记忆体ht融合了长期记忆和短期记忆。<br><img src="https://i.loli.net/2021/02/24/AMeZoga72HpNWvB.png" alt="image.png"></p>
<p>更新门和重置门都是0-1.<br>前向传播时，是计算的记忆体计算公式，计算出每个时刻的ht值</p>
<h2 id="TF描述GRU层"><a href="#TF描述GRU层" class="headerlink" title="TF描述GRU层"></a>TF描述GRU层</h2><p><strong>tf.keras.layers.GRU(记忆体个数,return_sequences=是否返回输出)</strong>   </p>
<ul>
<li>return_sequences 默认False<br>  一般最后一层是False，中间层是True</li>
</ul>
<p><img src="https://i.loli.net/2021/02/24/8v3BRiqxdUwZWEn.png" alt="image.png">   </p>
<h2 id="GRU实现股票预测-1"><a href="#GRU实现股票预测-1" class="headerlink" title="GRU实现股票预测"></a>GRU实现股票预测</h2><p>同LSTM,只需更改   </p>
<ul>
<li>import 改为  GRU  </li>
<li>model 处改为上图代码  </li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/22/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/22/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">经典卷积网络</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-02-22 20:31:28" itemprop="dateCreated datePublished" datetime="2021-02-22T20:31:28+08:00">2021-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-23 19:22:13" itemprop="dateModified" datetime="2021-02-23T19:22:13+08:00">2021-02-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><p>通过共享卷积核减少了网络的参数<br>在统计卷积神经网络层数时，一般只统计卷积计算层和全连接层<br>LeNet一共有五层网络<br>两层卷积、三层连续的全连接    </p>
<ul>
<li>卷积层1<ul>
<li>C(核:6<em>5</em>5,步长：1，填充：valid)</li>
<li>B(None)</li>
<li>A(sigmoid)</li>
<li>P(max,核:2*2,步长:2,填充:valid)</li>
<li>D(None)</li>
</ul>
</li>
<li>卷积层2<ul>
<li>C(核:16<em>5</em>5,步长：1，填充：valid)</li>
<li>B(None)</li>
<li>A(sigmoid)</li>
<li>P(max,核:2*2,步长:2,填充:valid)</li>
<li>D(None)</li>
</ul>
</li>
<li>全连接层<ul>
<li>Flatten</li>
<li>Dense(神经元:120, 激活:sigmoid)</li>
<li>Dense(神经元:84, 激活:sigmoid)</li>
<li>Dense(神经元:10, 激活:softmax)   </li>
</ul>
</li>
</ul>
<pre><code>class LeNet5(Model):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.c1 = Conv2D(filters=6, kernel_size=(5, 5),
                        activation=&apos;sigmoid&apos;)
        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2)

        self.c2 = Conv2D(filters=16, kernel_size=(5, 5),
                        activation=&apos;sigmoid&apos;)
        self.p2 = MaxPool2D(pool_size=(2, 2), strides=2)

        self.flatten = Flatten()
        self.f1 = Dense(120, activation=&apos;sigmoid&apos;)
        self.f2 = Dense(84, activation=&apos;sigmoid&apos;)
        self.f3 = Dense(10, activation=&apos;softmax&apos;)

    def call(self, x):
        x = self.c1(x)
        x = self.p1(x)

        x = self.c2(x)
        x = self.p2(x)

        x = self.flatten(x)
        x = self.f1(x)
        x = self.f2(x)
        y = self.f3(x)
        return y


model = LeNet5()</code></pre><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>使用激活函数<code>relu</code>，提升了训练速度<br>使用<code>Dropout</code>，缓解了过拟合<br>AlexNet一共有八层<br>五层卷积，三层全连接   </p>
<ul>
<li><p>卷积层1</p>
<ul>
<li>C(核:96<em>3</em>3,步长：1，填充：valid)</li>
<li>B(Yes,LRN)  # LRN是局部响应标准化，近年使用少</li>
<li>A(relu)  </li>
<li>P(max,核:3*3,步长:2)</li>
<li>D(None)</li>
</ul>
</li>
<li><p>卷积层2</p>
<ul>
<li>C(核:256<em>3</em>3,步长：1，填充：valid)</li>
<li>B(Yes,LRN)  # LRN是局部响应标准化，近年使用少</li>
<li>A(relu)  </li>
<li>P(max,核:3*3,步长:2)</li>
<li>D(None)</li>
</ul>
</li>
<li><p>卷积层3</p>
<ul>
<li>C(核:384<em>3</em>3,步长：1，填充：same)</li>
<li>B(None)</li>
<li>A(relu)  </li>
<li>P(None)</li>
<li>D(None)</li>
</ul>
</li>
<li><p>卷积层4</p>
<ul>
<li>C(核:384<em>3</em>3,步长：1，填充：same)</li>
<li>B(None)</li>
<li>A(relu)  </li>
<li>P(None)</li>
<li>D(None)</li>
</ul>
</li>
<li><p>卷积层5</p>
<ul>
<li>C(核:256<em>3</em>3,步长：1，填充：same)</li>
<li>B(None)</li>
<li>A(relu)  </li>
<li>P(max,核:3*3,步长:2)</li>
<li>D(None)</li>
</ul>
</li>
<li><p>全连接层   </p>
<ul>
<li><p>Flatten</p>
</li>
<li><p>Dense(神经元:2048,激活:relu,Dropout:0.5)</p>
</li>
<li><p>Dense(神经元:2048,激活:relu,Dropout:0.5)</p>
</li>
<li><p>Dense(神经元:10,激活:softmax)   </p>
<pre><code>class AlexNet8(Model):
    def __init__(self):
        super(AlexNet8, self).__init__()
        self.c1 = Conv2D(filters=96, kernel_size=(3, 3))
        self.b1 = BatchNormalization()
        self.a1 = Activation(&apos;relu&apos;)
        self.p1 = MaxPool2D(pool_size=(3, 3), strides=2)

        self.c2 = Conv2D(filters=256, kernel_size=(3, 3))
        self.b2 = BatchNormalization()
        self.a2 = Activation(&apos;relu&apos;)
        self.p2 = MaxPool2D(pool_size=(3, 3), strides=2)

        self.c3 = Conv2D(filters=384, kernel_size=(3, 3), padding=&apos;same&apos;,
                        activation=&apos;relu&apos;)

        self.c4 = Conv2D(filters=384, kernel_size=(3, 3), padding=&apos;same&apos;,
                        activation=&apos;relu&apos;)

        self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding=&apos;same&apos;,
                        activation=&apos;relu&apos;)
        self.p3 = MaxPool2D(pool_size=(3, 3), strides=2)

        self.flatten = Flatten()
        self.f1 = Dense(2048, activation=&apos;relu&apos;)
        self.d1 = Dropout(0.5)
        self.f2 = Dense(2048, activation=&apos;relu&apos;)
        self.d2 = Dropout(0.5)
        self.f3 = Dense(10, activation=&apos;softmax&apos;)

    def call(self, x):
        x = self.c1(x)
        x = self.b1(x)
        x = self.a1(x)
        x = self.p1(x)

        x = self.c2(x)
        x = self.b2(x)
        x = self.a2(x)
        x = self.p2(x)

        x = self.c3(x)

        x = self.c4(x)

        x = self.c5(x)
        x = self.p3(x)

        x = self.flatten(x)
        x = self.f1(x)
        x = self.d1(x)
        x = self.f2(x)
        x = self.d2(x)
        y = self.f3(x)
        return y</code></pre></li>
</ul>
</li>
</ul>
<pre><code>model = AlexNet8()    </code></pre><h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><p><img src="https://i.loli.net/2021/02/22/jwRvAmQIEqTLlX5.png" alt="image.png"></p>
<h1 id="InceptionNet"><a href="#InceptionNet" class="headerlink" title="InceptionNet"></a>InceptionNet</h1><p>InceptionNet引入了Inception结构块<br>在同一层网络内使用不同尺寸的卷积核，提升了模型感知力<br>使用了批标准化，缓解了梯度消失<br><img src="https://i.loli.net/2021/02/22/1yXFTZmadBczhPA.png" alt="image.png"><br>Inception结构块在同一层网络中使用了多个尺寸的卷积核，可以提取不同尺寸的特征<br>通过<code>1*1</code>卷积核作用到输入特征的每个像素点，通过设定少于输入特征图深度的<code>1*1</code>卷积核个数，减少了输出特征图深度，起到了降维的作用，减少了参数量核计算量<br>Inception结构块包含四个分支，分别</p>
<ul>
<li>经过<code>1*1</code>卷积核输出到卷积连接器；</li>
<li>经过<code>1*1</code>卷积核配合<code>3*3</code>卷积核输出到卷积连接器；</li>
<li>经过<code>1*1</code>卷积核配合<code>5*5</code>卷积核输出到卷积连接器；</li>
<li>经过<code>3*3</code>最大池化核配合<code>1*1</code>卷积核输出到卷积连接器；  </li>
</ul>
<p>送到卷积连接器的特征数据尺寸相同<br>卷积连接器会把收到的这四路特征数据按深度方向拼接，形成Inception结构块的输出<br>由于Inception结构块中的卷积操作均采用了先卷积，再BN，再relu激活函数(CBA),则定义为一个新的类ConvBNRelu   </p>
<pre><code>class ConvBNRelu(Model):
    def __init__(self, ch, kernelsz=3, strides=1, padding=&apos;same&apos;):
        super(ConvBNRelu, self).__init__()
        self.model = tf.keras.models.Sequential([
            Conv2D(ch, kernelsz, strides=strides, padding=padding),
            BatchNormalization(),
            Activation(&apos;relu&apos;)
        ])

    def call(self, x):
        x = self.model(x, training=False) #在training=False时，BN通过整个训练集计算均值、方差去做批归一化，training=True时，通过当前batch的均值、方差去做批归一化。推理时 training=False效果好
        return x 
class InceptionBlk(Model):
    def __init__(self, ch, strides=1):
        super(InceptionBlk, self).__init__()
        self.ch = ch
        self.strides = strides
        self.c1 = ConvBNRelu(ch, kernelsz=1, strides=strides)  # 第一个分支
        self.c2_1 = ConvBNRelu(ch, kernelsz=1, strides=strides) #第二个分支的第一个卷积核
        self.c2_2 = ConvBNRelu(ch, kernelsz=3, strides=1)       #第二个分支的第二个卷积核
        self.c3_1 = ConvBNRelu(ch, kernelsz=1, strides=strides)     #以此类推
        self.c3_2 = ConvBNRelu(ch, kernelsz=5, strides=1)
        self.p4_1 = MaxPool2D(3, strides=1, padding=&apos;same&apos;)
        self.c4_2 = ConvBNRelu(ch, kernelsz=1, strides=strides)

    def call(self, x):
        # x1,x2_2,x3_2,x4_2 是四个分支的输出
        x1 = self.c1(x)
        x2_1 = self.c2_1(x)
        x2_2 = self.c2_2(x2_1)
        x3_1 = self.c3_1(x)
        x3_2 = self.c3_2(x3_1)
        x4_1 = self.p4_1(x)
        x4_2 = self.c4_2(x4_1)
        # concat along axis=channel             
        x = tf.concat([x1, x2_2, x3_2, x4_2], axis=3)      #使用该函数将他们堆叠在一起，axis=3指定堆叠的维度是沿深度方向
        return x


    class Inception10(Model):
        def __init__(self, num_blocks, num_classes, init_ch=16, **kwargs):
            super(Inception10, self).__init__(**kwargs)
            self.in_channels = init_ch
            self.out_channels = init_ch
            self.num_blocks = num_blocks
            self.init_ch = init_ch
            self.c1 = ConvBNRelu(init_ch)
            self.blocks = tf.keras.models.Sequential()
            for block_id in range(num_blocks):
                for layer_id in range(2):
                    if layer_id == 0:
                        block = InceptionBlk(self.out_channels, strides=2)
                    else:
                        block = InceptionBlk(self.out_channels, strides=1)
                    self.blocks.add(block)
                # enlarger out_channels per block
                self.out_channels *= 2
            self.p1 = GlobalAveragePooling2D()
            self.f1 = Dense(num_classes, activation=&apos;softmax&apos;)

        def call(self, x):
            x = self.c1(x)
            x = self.blocks(x)
            x = self.p1(x)
            y = self.f1(x)
            return y</code></pre><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/22/Cifar10%E6%95%B0%E6%8D%AE%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/22/Cifar10%E6%95%B0%E6%8D%AE%E9%9B%86/" class="post-title-link" itemprop="url">Cifar10数据集</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-02-22 19:25:50 / Modified: 20:30:31" itemprop="dateCreated datePublished" datetime="2021-02-22T19:25:50+08:00">2021-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<p>title: Cifar10数据集<br>date: 2021-02-22 19:25:50<br>tags:</p>
<hr>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Cifar10数据集：    </p>
<ul>
<li>5万张32*32像素点的十分类彩色图片和标签，用于训练    </li>
<li>1万张32*32像素点的十分类彩色图片和标签，用于测试   </li>
</ul>
<p><img src="https://i.loli.net/2021/02/22/DNhYKurdJsQwvCq.png" alt="image.png"><br>这十个类别分别对应标签0、1、3、4、5、6、7、8、9    </p>
<pre><code># 查看数据集里的图片
# 也就是我们的输入x     

plt.imshow(x_train[0])  
plt.show()    


# 输入图片表达为像素矩阵   
&gt; x_train[0]
# 是一个(32,32,3),三通道的三维数组 
&gt; x_test.shape
# (10000,32,32,3)

&gt; y_train[0]
...
6

# 这应该算是索引值为6，y应该要有一个对应的列表即可    </code></pre><h1 id="搭建卷积网络示例"><a href="#搭建卷积网络示例" class="headerlink" title="搭建卷积网络示例"></a>搭建卷积网络示例</h1><ul>
<li><p>6个5*5的卷积层   </p>
</li>
<li><p>2个2*2的池化层    </p>
</li>
<li><p>两个隐藏层   </p>
<ul>
<li>第一个128个神经元   </li>
<li>第二个10个(与输出对应)   </li>
</ul>
</li>
</ul>
<pre><code># C卷积(Convolutional)   
Conv2D(filters=6,kernel_size=(5,5),padding=&apos;same&apos;),

# B批标准化(BN)  
BatchNormalization(),

# A激活(Activation)   
Actication(&apos;relu&apos;),   

# P池化(Pooling)   
MaxPool2D(pool_size=(2,2),strides=2,padding=&apos;same&apos;),

# D舍弃(Dropout)   
Dropout(0.2)   </code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/22/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/22/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA/" class="post-title-link" itemprop="url">卷积网络搭建</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-02-22 14:03:08 / Modified: 17:16:56" itemprop="dateCreated datePublished" datetime="2021-02-22T14:03:08+08:00">2021-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="卷积计算过程"><a href="#卷积计算过程" class="headerlink" title="卷积计算过程"></a>卷积计算过程</h1><p>全连接网络(也写作：全连接NN)中每一个神经元小球与前后相邻层的每一个神经元都有连接关系，输入是特征，输出是预测的结果<br>当网络参数过多，隐藏层数增加，很容易使模型过拟合。<br>为了减少待训练参数，则先对原始图片进行特征提取，再把提取到的特征送到全连接网络，让全连接网络输出识别结果   </p>
<p><strong>卷积计算是一种有效的特征提取方法</strong><br>一般用一个正方形的卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项，得到输出特征的一个像素点。<br><img src="https://i.loli.net/2021/02/22/TBMPohCsjU3ZNAF.png" alt="image.png">    </p>
<p><strong>🎱补充</strong>   </p>
<ul>
<li>图像的深度<br>图像是由一个个像素点构成，而计算机是用二进制来进行储存，每一个像素点所占用的bit位数，就是图像的深度。   <ul>
<li>二值图像<br>像素点取值为0或1，也就是黑白两色.像素点所占位数为1位，则深度为1  </li>
<li>灰度图像<br>像素点取值为0-255，2^8=256，所以深度是8   </li>
</ul>
</li>
<li>图像的通道<br>24位图，也就是2^24，我们用8位代表一种颜色，那么每一种颜色都是0-255的范围内，每一个像素点为(0-255,0-255,0-255),这就是三通道RGB    </li>
</ul>
<p><strong>卷积核的通道数与输入特征图的通道数一致</strong><br>如果输入特征是单通道灰图，用深度为1的单通道卷积核<br>如果输入特征是三通道彩色图，用3<em>3</em>3的卷积核或5<em>5</em>3的卷积核<br>要想让卷积核与输入特征图对应点匹配上，必须让卷积核的深度与输入特征图的深度一致<br><strong>输入特征图的深度(channel数),决定了当前层卷积核的深度:</strong><br>(上述加粗的就是原文的表述，我觉得应该不是指图像的深度，因该是图像的通道数)<br>每个卷积核在卷积计算后会得到一张输出特征图，所以当前有几个卷积核，就有几张输出特征图。<br><strong>当前层卷积核的个数，决定了当前层输出特征图的深度</strong><br>可以通过多用几个卷积核提高该层的特征提取能力<br><img src="https://i.loli.net/2021/02/22/jZkKz6gTyHlE5MQ.png" alt="image.png">  </p>
<p>每一个小颗粒都存储着待训练参数，在执行卷积计算时，这些参数是不变的。而在反向传播中，这些参数是随着梯度下降跟新的<br>卷积就是利用立体卷积核实现了参数的空间共享    </p>
<blockquote>
<p>You can imagine convolution as the mixing of information. Imagine two buckets full of information which are poured into one single bucket and then mixed according to a specific rule. Each bucket of information has its own recipe, which describes how the information in one bucket mixes with the other. So convolution is an orderly procedure where two sources of information are intertwined.</p>
</blockquote>
<h1 id="感受野Receptive-Field"><a href="#感受野Receptive-Field" class="headerlink" title="感受野Receptive Field"></a>感受野Receptive Field</h1><p><strong>定义：</strong> 卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小<br><img src="https://i.loli.net/2021/02/22/jCZ6egWkyBzMDL5.png" alt="image.png"><br>可以看到原始的<code>5*5</code>的图用黄色的<code>3*3</code>卷积核作用，则得到了一个<code>3*3</code>的输出特征图，图上的每个像素点映射到原始输入图片是<code>3*3</code>的区域，所以其<strong>感受野是3</strong><br><img src="https://i.loli.net/2021/02/22/F6ayAlHZi2ObqTt.png" alt="image.png"><br>而如果再对这个<code>3*3</code>的输出特征图用<code>3*3</code>的卷积核作用，输出的是一个<code>1*1</code>的输出特征图，而这个输出特征图的<strong>感受野是5</strong>，因为要映射到原始图片，先映射一次，是上一个输出特征图的所有，则再映射就是原始特征图的所有了。<br><img src="https://i.loli.net/2021/02/22/n4uvsiVSPxJkFB7.png" alt="image.png"><br>而如果<code>5*5</code>的原始图直接用<code>5*5</code>的卷积核作用，得到的是<code>1*1</code>的输出特征图，<strong>感受野也是5</strong>    </p>
<p>也就是，两层的<code>3*3</code>卷积核和一层的<code>5*5</code>卷积核作用得到的输出特征图的感受野都是5；所以这两个的特征提取能力是一样的。<br>由此我们可以看到的是，特征提取能力的可以通过感受野的大小来判断。<br>那么是选择<code>两个3*3</code>还是<code>一个5*5</code>呢？<br>此时则考虑他们所承载的<strong>待训练参数量</strong>和<strong>计算量</strong>了。<br><img src="https://i.loli.net/2021/02/22/gTzqi91PBx5puWt.png" alt="image.png"><br><strong>🎱补充</strong><br>经过卷积核作用后的输出特征图的像素点个数的计算公式。<br>假设，输入特征图的宽、高为x,卷积核为m<em>m，则经过卷积核作用后的输出特征图一共有<code>(x-m+1)^2</code>个像素点<br>得到每一个像素点要进行<code>m*m</code>次乘加运算，所以总计算量为```m*m</em>(x-m+1)^2```      </p>
<p>所以在神经网络计算中常使用<code>两层3*3卷积核</code>替换<code>5*5卷积核</code>的原因   </p>
<h1 id="全零填充"><a href="#全零填充" class="headerlink" title="全零填充"></a>全零填充</h1><p>使卷积计算保持输入特征图尺寸不变，则用全零填充<br><img src="https://i.loli.net/2021/02/22/nXliHJ4Ivx73GDQ.png" alt="image.png"><br>也就是，输出特征图的维度计算公式为<code>VALID</code>部分。   </p>
<h1 id="描述卷积计算层"><a href="#描述卷积计算层" class="headerlink" title="描述卷积计算层"></a>描述卷积计算层</h1><p><img src="https://i.loli.net/2021/02/22/ALKBIh94GQErUzi.png" alt="image.png"><br><strong>🎱补充</strong><br>BN是指Batch Normalization,特就是批量标准化处理。  </p>
<h1 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h1><p>神经网络对0附近的数据更敏感，但会存在一些特征值偏离0的情况，所以就有了<code>标准化</code><br><strong>标准化：</strong>使数据符合以0为均值，以1为标准差的分布<br><strong>批标准化：</strong>对一小批数据(batch)，标准化处理<br>BN常用在卷积操作和激活操作之间<br><img src="https://i.loli.net/2021/02/22/kIOLWd6aJtubGRq.png" alt="image.png"><br>批标准化操作会让每个像素点进行<code>减均值并处以标准差</code>的自更新计算<br><img src="https://i.loli.net/2021/02/22/aibSC7FtvhjTJmG.png" alt="image.png"><br>通过BN操作，使得特征数据集中在激活函数线性变化区间，使得就算特征数据变化小，也能明显地提现到激活函数输出中，提升了激活函数对输入数据地区分力<br>但这样简单的标准化，使得特征数据完全满足标准正态分布，集中在激活函数中心的线性区域，是激活函数丧失；了非线性特性，因此在BN操作中为每个卷积核引入了两个可训练参数，缩放因子γ和偏移因子β<br>这两个因子会与其他带训练参数一同被训练优化，是标准正态分布后的特征数据通过缩放因子和偏移因子优化了特征数据分布的宽窄和偏移量，保证了网络的非线性表达力<br><img src="https://i.loli.net/2021/02/22/KA1q8HGwh9oFpYb.png" alt="image.png">   </p>
<h1 id="池化Pooling"><a href="#池化Pooling" class="headerlink" title="池化Pooling"></a>池化Pooling</h1><p><strong>池化用于减少特征数据量</strong><br>池化的主要方法   </p>
<ul>
<li>最大池化<br>可提取图片纹理<br>因为提取了主要特征，所以相当于纹理  </li>
<li>均值池化<br>可保留背景特征。<br><img src="https://i.loli.net/2021/02/22/hoBeALYsGSuyM2m.png" alt="image.png"><br>如图所示，用了一个2<em>2的池化核，对输入图片进行了以2为步长的池化。<br>也就是我用一个2</em>2的方框，框住了图片里对应的四个像素点，通过池化方法，比如最大池化，选择这四个像素点中最大的数作为输出的像素点值，或者均值池化，取这四个像素点的均值作为输出的像素点值，得到我们经过池化后的原图大小的四分之一。<br><img src="https://i.loli.net/2021/02/22/DZgYfNi3HB4b8Pp.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/22/6udcfhL4DPYVS5H.png" alt="image.png"></li>
</ul>
<h1 id="舍弃Dropout"><a href="#舍弃Dropout" class="headerlink" title="舍弃Dropout"></a>舍弃Dropout</h1><p>为了缓解神经网络过拟合，在神经网络训练时，将隐藏层的一部分神经元按照一定概率从神经网络中暂时舍弃。在神经网络使用时，被舍弃的神经元恢复连接。   </p>
<p><img src="https://i.loli.net/2021/02/22/64PE7DiGgNWXfM5.png" alt="image.png">    </p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络就是借助卷积核对输入特征进行特征提取，提取后送入全连接网络进行识别预测<br><img src="https://i.loli.net/2021/02/22/FrtYCqnyZcmDhT6.png" alt="image.png">   </p>
<p><img src="https://i.loli.net/2021/02/22/qCKZTbpyrPXxvU4.png" alt="image.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1%E5%8A%9F%E8%83%BD%E6%89%A9%E5%B1%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1%E5%8A%9F%E8%83%BD%E6%89%A9%E5%B1%95/" class="post-title-link" itemprop="url">神经网络八股功能扩展</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-02-21 20:02:20" itemprop="dateCreated datePublished" datetime="2021-02-21T20:02:20+08:00">2021-02-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-22 13:53:29" itemprop="dateModified" datetime="2021-02-22T13:53:29+08:00">2021-02-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="自制数据集"><a href="#自制数据集" class="headerlink" title="自制数据集"></a>自制数据集</h1><p>意思就是，若我有一堆图片，一个标签文本，如何提取做出我们的<code>x_train,y_train</code><br>首先我们将标签文本整理，第一列是图片名称，第二列就是该图片对应的标签。<br>为什么是图片名称？<br>因为，我们要提取出<code>x,y</code>也就是，我们从一个图片中提取了它的灰度值，要能够与y对应。<br>而如何提取一个图片的像素值？<br>导入<code>PIL</code>库中的<code>Image</code>,用<code>Imag.open(图片路径)</code>读入图片，<code>np.array(img.convert(&#39;L&#39;))</code>转变为灰度值的<code>np.array</code>格式，此时其实就拿到了我们的x，图片对应的灰度值，那么我们还应该做归一化处理<code>img/255</code><br>此时我们就得到了我们的x，那么要与y对应，就要保证在<code>标签的txt</code>文档中，名字与标签一一对应，再在读取图片时图片所在目录<code>path</code>加上图片名字就是图片的<code>path</code>。    </p>
<p>🎆具体代码如下<br><img src="https://i.loli.net/2021/02/21/tcNxvV1ewHmJZnp.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/21/VXqHKwU9rFE41QI.png" alt="image.png"></p>
<h1 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h1><p>数据增强可以帮助扩展数据集<br>对图像的增强，就是对图像的简单形变。<br>Tensorlow2有数据增强函数<br><img src="https://i.loli.net/2021/02/21/6FDYsyQNM9ohanb.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/21/EPkrO7dFKetN8y9.png" alt="image.png"><br>这个相当于前面是一种设置，然后用<code>fit()</code>把设置的参数放在我们的训练集中<code>x_train</code><br>而这里的<code>imag_gen_train_fit(x_train)</code>需要输入一个四位数据，所以要对<code>x_train.reshape(x_train.shape[0],28,28,1)</code><br>也就是将(60000,28,28) -&gt; (60000,28,28,1)<br><img src="https://i.loli.net/2021/02/21/v96z4hRuZDcOApm.png" alt="image.png">    </p>
<h1 id="断点续训"><a href="#断点续训" class="headerlink" title="断点续训"></a>断点续训</h1><p><strong>定义：</strong> 在进行神经网络训练过程中由于一些因素导致训练无法进行，需要保存当前的训练结果，下次接着训练    </p>
<h2 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h2><p>可直接使用tensorflow给出的函数<code>load_weights(路径文件名)</code>读取已有模型参数<br>先定义出存放模型的路径和文件名，并命名为ckpt文件<code>checkpoint_save_path = &quot;./checkpoint/mnist.ckpt&quot;</code><br><strong>🎱补充</strong> ckpt文件用于存放<code>weights、biases、gradients等</code>变量<br>在生成<code>ckpt</code>文件时会同步生成索引表，所以可以通过判断是否已有索引表，就知道是不是已经保存过模型参数了<code>if os.path.exists(checkpoint_save_path + &#39;.index&#39;):</code><br>如果有了索引表，就可以调用<code>model.load_weights(checkpoint_save_path)</code>函数，来读取模型参数。</p>
<h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><p>保存模型参数可以使用tensorflow的回调函数直接保存训练出来的参数。     </p>
<pre><code>tf.keras.callbacks.ModelCheckpoint(
    filepath = 路径文件名,   
    save_weights_only =True/False, #是否只保留模型参数
    save_best_only=True/False   #是否只保留最优结果
)
history = model.fit(callbacks=[cp_callback])</code></pre><p><img src="https://i.loli.net/2021/02/21/obsF9EQxZ3KzCNM.png" alt="image.png"></p>
<p><img src="https://i.loli.net/2021/02/21/lpXTjmz4kU29oZ5.png" alt="image.png"></p>
<h2 id="参数提取"><a href="#参数提取" class="headerlink" title="参数提取"></a>参数提取</h2><p><strong>把参数存入文本</strong><br>Tensorflow中可以用<code>model.trainable_variables</code>返回当前模型中所有可训练参数<br>可以用<code>print</code>函数打印出这些可训练参数<br>但是如果直接<code>print</code>，很多参数会被省略号替换掉,可以通过<code>np.set_printoptions(threshold=超过多少省略显示)</code>设置打印效果<br>设置为<code>np.set_printoptions(threshold=np.inf)</code>则，所有都不省略，都打印<br>这时，再<code>print(model.trainable_variables)</code>就可以打印出所有可训练参数了    </p>
<h2 id="acc-loss可视化"><a href="#acc-loss可视化" class="headerlink" title="acc/loss可视化"></a>acc/loss可视化</h2><p>在<code>model.fit()</code>中记录了<code>训练集loss、测试集loss、训练集准确率sparse_categorical_accuracy、测试集准确率val_sparse_categorical_accuracy</code>    </p>
<pre><code>history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1,
                callbacks=[cp_callback])


# 提取了model.fit函数在执行过程中，保存的训练集准确率等
acc = history.history[&apos;sparse_categorical_accuracy&apos;]
val_acc = history.history[&apos;val_sparse_categorical_accuracy&apos;]
loss = history.history[&apos;loss&apos;]
val_loss = history.history[&apos;val_loss&apos;]

plt.subplot(1, 2, 1)
plt.plot(acc, label=&apos;Training Accuracy&apos;)
plt.plot(val_acc, label=&apos;Validation Accuracy&apos;)
plt.title(&apos;Training and Validation Accuracy&apos;)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label=&apos;Training Loss&apos;)
plt.plot(val_loss, label=&apos;Validation Loss&apos;)
plt.title(&apos;Training and Validation Loss&apos;)
plt.legend()
plt.show()</code></pre><p><img src="https://i.loli.net/2021/02/22/Z2tRlU8SGAc3p4H.png" alt="image.png"></p>
<h2 id="给图识物"><a href="#给图识物" class="headerlink" title="给图识物"></a>给图识物</h2><p>意思应该是，我现在有一个模型了，那么我要如何把我要测试的加载进去，得到相应的结果<br>运用<code>predict(输入特征,batch_size=整数)</code>返回前向传播计算结果<br>    # 复现模型<br>    model = tf.keras.models.Sequential([<br>        tf.keras.layers.Flatten(),<br>        tf.keras.layers.Dense(128,activation=’relu’),<br>        tf.keras.layers.Dense(10,activation=’softmax’)<br>    ])</p>
<pre><code># 加载参数   
model.load_weights(model_save_path)   

# 预测结果
result = model.predict(x_predict)    </code></pre><p><strong>代码思路:</strong><br>目的是，我现在训练了一个模型，有一堆测试图片，来测试这个神经网络模型<br>首先，我要加载出我的网络，也就是给出其地址，加载出参数，然后要喂入我的测试集的数据  </p>
<pre><code>model_save_path = &apos;./checkpoint/mnist.ckpt&apos;

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=&apos;relu&apos;),
    tf.keras.layers.Dense(10, activation=&apos;softmax&apos;)])

model.load_weights(model_save_path)</code></pre><p>那么我现在时随便给的图片，大小，像素点的值也与我原本的不一样。<br>拿mnist举例，给出的全是28*28的黑底白字的图片，那么我对我的图片进行预处理。    </p>
<ul>
<li><p>法一<br>凡是，灰度后值小于200的全设为255，也就是设为白色；凡是大于的设为黑色，则反转了    </p>
</li>
<li><p>法二<br>直接反，<code>img_arr = 255-img_arr</code>    </p>
<h1 id="对图片进行处理"><a href="#对图片进行处理" class="headerlink" title="对图片进行处理"></a>对图片进行处理</h1><p>  preNum = int(input(“input the number of test pictures:”))</p>
<p>  for i in range(preNum):</p>
<pre><code>image_path = input(&quot;the path of test picture:&quot;)
img = Image.open(image_path)
img = img.resize((28, 28), Image.ANTIALIAS)
img_arr = np.array(img.convert(&apos;L&apos;))</code></pre><p>  #第二种方法，直接颠倒</p>
<pre><code>img_arr = 255 - img_arr</code></pre><p>  #第一种方法，模糊化，全部变为黑白    </p>
<pre><code>for i in range(28):
    for j in range(28):
        if img_arr[i][j] &lt; 200:
            img_arr[i][j] = 255
        else:
            img_arr[i][j] = 0</code></pre></li>
</ul>
<pre><code>#最后记得归一化处理   
img_arr = img_arr / 255.0


#img.resize((width, height),Image.ANTIALIAS)
第二个参数：
    Image.NEAREST ：低质量
    Image.BILINEAR：双线性
    Image.BICUBIC ：三次样条插值
    Image.ANTIALIAS：高质量</code></pre><p>此时我们的图片数据也准备好了，但这里要注意，我们在训练集中喂入的是三维，所以要加一维<code>x_predict = img_arr[tf.newaxis, ...]</code><br><strong>最后完整版代码：</strong><br>    from PIL import Image<br>    import numpy as np<br>    import tensorflow as tf</p>
<pre><code>model_save_path = &apos;./checkpoint/mnist.ckpt&apos;

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=&apos;relu&apos;),
    tf.keras.layers.Dense(10, activation=&apos;softmax&apos;)])

model.load_weights(model_save_path)

preNum = int(input(&quot;input the number of test pictures:&quot;))

for i in range(preNum):
    image_path = input(&quot;the path of test picture:&quot;)
    img = Image.open(image_path)
    img = img.resize((28, 28), Image.ANTIALIAS)
    img_arr = np.array(img.convert(&apos;L&apos;))

    for i in range(28):
        for j in range(28):
            if img_arr[i][j] &lt; 200:
                img_arr[i][j] = 255
            else:
                img_arr[i][j] = 0

    img_arr = img_arr / 255.0
    x_predict = img_arr[tf.newaxis, ...]
    result = model.predict(x_predict)

    pred = tf.argmax(result, axis=1)

    print(&apos;\n&apos;)
    tf.print(pred)</code></pre><p><img src="https://i.loli.net/2021/02/22/P2Ad1vEB8ysTfxL.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/22/FJqbSR7eusfYUHx.png" alt="image.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/19/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/19/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B/" class="post-title-link" itemprop="url">Tensorflow学习笔记(四)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-02-19 14:02:50" itemprop="dateCreated datePublished" datetime="2021-02-19T14:02:50+08:00">2021-02-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-21 19:52:24" itemprop="dateModified" datetime="2021-02-21T19:52:24+08:00">2021-02-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul>
<li>提供六万张28*28像素点的0~9手写数字图片和标签，用于训练   </li>
<li>提供一万张28*28像素点的0~9手写数字图片和标签，用于测试 </li>
</ul>
<pre><code>#导入MNIST数据集  
mnist = tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test) =mnist.load_data()   

#将所有像素点(28*28=784)的灰度值作为输入特征，输入神经网络时，将数据拉伸为一维数组:   
tf.keras.layers.Flatten()    </code></pre><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><pre><code>import tensorflow as tf
from matplotlib import pyplot as plt

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 可视化训练集输入特征的第一个元素
plt.imshow(x_train[0], cmap=&apos;gray&apos;)  # 绘制灰度图
plt.show()

# 打印出训练集输入特征的第一个元素
print(&quot;x_train[0]:\n&quot;, x_train[0])
# 打印出训练集标签的第一个元素
print(&quot;y_train[0]:\n&quot;, y_train[0])

# 打印出整个训练集输入特征形状
print(&quot;x_train.shape:\n&quot;, x_train.shape)
# 打印出整个训练集标签的形状
print(&quot;y_train.shape:\n&quot;, y_train.shape)
# 打印出整个测试集输入特征的形状
print(&quot;x_test.shape:\n&quot;, x_test.shape)
# 打印出整个测试集标签的形状
print(&quot;y_test.shape:\n&quot;, y_test.shape)</code></pre><p>x未截完，可以看见，x是28<em>28的矩阵，每一个点，代表该处的灰度值<br>0表示黑色，255表示纯白色<br><img src="https://i.loli.net/2021/02/21/ws3MbZCXG4xVPi5.png" alt=""><br>y则是标签也就是该图片对应的数值是指多少。<br>可以看到，整个<code>x_train</code>是60000个28</em>28的矩阵，而<code>y_train</code>是60000个值，是一个列向量。<br><img src="https://i.loli.net/2021/02/21/HpIDnVXKeLSsh4B.png" alt="image.png"><br><img src="https://i.loli.net/2021/02/21/lT3OfDFC7H9tj5s.png" alt="image.png">   </p>
<h2 id="Sequential实现"><a href="#Sequential实现" class="headerlink" title="Sequential实现"></a>Sequential实现</h2><pre><code>import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  #归一化，使0-255的灰度值变成0-1之间的数值

model = tf.keras.models.Sequential([                #输入层是不用定义的
    tf.keras.layers.Flatten(),                      #把输入特征拉直为一维数组
    tf.keras.layers.Dense(128, activation=&apos;relu&apos;),      #隐藏层，激活函数relu
    tf.keras.layers.Dense(10, activation=&apos;softmax&apos;)    #输出层，用softmax使输出符合概率分布
])

#配置训练方法
model.compile(optimizer=&apos;adam&apos;,  #优化器
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),   #因为不是原始输出，而是用了概率分布，所以用false
            metrics=[&apos;sparse_categorical_accuracy&apos;]) #数据集标签是数值，神经网络输出是概率分布

model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
model.summary()</code></pre><h2 id="class实现"><a href="#class实现" class="headerlink" title="class实现"></a>class实现</h2><pre><code>import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras import Model

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0


class MnistModel(Model):
    def __init__(self):
        super(MnistModel, self).__init__()
        self.flatten = Flatten()
        self.d1 = Dense(128, activation=&apos;relu&apos;)
        self.d2 = Dense(10, activation=&apos;softmax&apos;)

    def call(self, x):
        x = self.flatten(x)
        x = self.d1(x)
        y = self.d2(x)
        return y


model = MnistModel()

model.compile(optimizer=&apos;adam&apos;,
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=[&apos;sparse_categorical_accuracy&apos;])

model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
model.summary()</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/19/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/19/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/" class="post-title-link" itemprop="url">Tensorflow学习笔记(三)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-02-19 09:45:00 / Modified: 14:01:50" itemprop="dateCreated datePublished" datetime="2021-02-19T09:45:00+08:00">2021-02-19</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>用Sequential可以搭建出上层输出就是下层输入的顺序网络结构，但无法写出一些带有跳连的非顺序网络结构。<br>此时选择用类class搭建神经网络结构<br><strong>🎱六步法</strong>   </p>
<ul>
<li>import</li>
<li>train，test</li>
<li>class MyModel(Model) model=MyModel   </li>
<li>model.compile</li>
<li>model.fit</li>
<li>model.summary</li>
</ul>
<h1 id="class类封装神经网络结构"><a href="#class类封装神经网络结构" class="headerlink" title="class类封装神经网络结构"></a>class类封装神经网络结构</h1><pre><code>✨
class MyModel(Model):
    def __init__(self):
        super(MyModel,self).__init__()
        定义网络结构块
    def call(self,x):
        调用网络结构块，实现前向传播
        return y
model = MyModel()
✨



🎆#鸢尾花   

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import Model
from sklearn import datasets
import numpy as np

x_train = datasets.load_iris().data
y_train = datasets.load_iris().target

np.random.seed(116)
np.random.shuffle(x_train)
np.random.seed(116)
np.random.shuffle(y_train)
tf.random.set_seed(116)

class IrisModel(Model):
    def __init__(self):
        super(IrisModel, self).__init__()
        self.d1 = Dense(3, activation=&apos;softmax&apos;, kernel_regularizer=tf.keras.regularizers.l2())

    def call(self, x):
        y = self.d1(x)
        return y

model = IrisModel()

model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=[&apos;sparse_categorical_accuracy&apos;])

model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)
model.summary()</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/18/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/18/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/" class="post-title-link" itemprop="url">Tensorflow学习笔记(二)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-02-18 19:23:12 / Modified: 20:48:59" itemprop="dateCreated datePublished" datetime="2021-02-18T19:23:12+08:00">2021-02-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="使用八股搭建神经网络"><a href="#使用八股搭建神经网络" class="headerlink" title="使用八股搭建神经网络"></a>使用八股搭建神经网络</h1><p><strong>用Tensorflow API：tf.keras搭建网络八股</strong>   </p>
<p><strong>🎱六步法</strong></p>
<ul>
<li>import相关模块   </li>
<li>确定训练集和测试集<br>  指定训练集的输入特征和标签；<br>  指定测试集的输入特征和标签。   </li>
<li>在<code>Sequential()</code>中搭建网络结构，逐层描述每层网络<br>  相当于走一遍前向传播   </li>
<li>在<code>compile()</code>中配置训练方法<br>  告知训练时选择哪种优化器；<br>  选择哪种损失函数；<br>  选择哪种评测指标；   </li>
<li>在<code>fit()</code>中执行训练过程<br>  告知训练集和测试集的输入特征和标签；<br>  每个batch是多少；<br>  迭代多少次训练集；  </li>
<li>用<code>summary()</code>打印出网络的结构和参数统计  </li>
</ul>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential()"></a>Sequential()</h2><pre><code># 描述从输入层到输出层，每一层的网络结构
✨model = tf.keras.models.Sequential([网络结构])     

# 每一层的网络结构，可以是👇  

#拉直层  
#该层不含计算，只是形状转换
#把输入特征拉直变成一维数组
tf.keras.layers.Flatten()


#全连接层  
tf.keras.layers.Dense(神经元个数,activation=&apos;激活函数&apos;,kernel_regularizer=那种正则化)   
#activation(以字符串形式给出)可选: relu、softmax、sigmoid、tanh   
#kernel_regularizer 可选: tf.keras.regularizers.l1()、tf.keras.regularizers.l2()   

#卷积层  
tf.keras.layers.Conv2D(filters=卷积核个数,kernel_size=卷积核尺寸,strides=卷积步长,padding=&quot;valid&quot;or&quot;same&quot;)    


#LSTM层(循环神经网络层)  
tf.keras.layers.LSTM()</code></pre><h2 id="compile"><a href="#compile" class="headerlink" title="compile()"></a>compile()</h2><pre><code>✨model.complie(optimizer=优化器,loss=损失函数,metrics=[&quot;准确率&quot;])



#优化器optimizer可以是字符串形式给出的👇   
&gt;&gt;&apos;sgd&apos; or tf.keras.optimizers.SGD(lr=学习率,momentum=动量参数) 

&gt;&gt;&apos;adagrad&apos; pr tf.keras.optimizers.Adagrad(lr=*)

&gt;&gt;&apos;adadelta&apos; or tf.keras.optimizers.Adadelta(lr=*)   



#损失函数loss👇   
&gt;&gt;&apos;mse&apos; or tf.keras.losses.MeanSquaredError()  

&gt;&gt;&apos;sparse_categorical_crossentropy&apos; or tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)  
#👆这个函数的参数是在询问是否原始输出；
#神经网络预测结果输出前经过概率分布，就不是原始输出，此时就是False



#网络评测指标Metrics👇:  
&gt;&gt;&apos;accuracy&apos;:y_和y都是数值，如y_=[1],y=[1]  

&gt;&gt;&apos;categorical_accuracy&apos;:y_和y都是独热码(概率分布),如y_=[0,1,0],y=[0.256,0.695,0.048] 

&gt;&gt;&apos;sparse_categorical_accuracy&apos;:y_是数值，y是独热码(概率分布),如y_=[1],y=[0.256,0.695.0.048]   </code></pre><h2 id="fit"><a href="#fit" class="headerlink" title="fit()"></a>fit()</h2><pre><code>✨model.fit(训练集的输入特征,训练集的标签,batch_size= , epochs= ,validation_data=(测试集的输入特征,测试集的标签),validation_split=从训练集划分多少比例给测试集,validation_freq=多少次epoch测试一次)   </code></pre><ul>
<li>validation_data与validation_split两者选其一  </li>
</ul>
<h2 id="model-summay"><a href="#model-summay" class="headerlink" title="model.summay()"></a>model.summay()</h2><p>打印出网络的结构和参数统计<br><img src="https://i.loli.net/2021/02/18/1nCZIvTzxstSBQM.png" alt="image.png">   </p>
<h2 id="鸢尾花示例"><a href="#鸢尾花示例" class="headerlink" title="鸢尾花示例"></a>鸢尾花示例</h2><pre><code>import tensorflow as tf
from sklearn import datasets
import numpy as np

x_train = datasets.load_iris().data
y_train = datasets.load_iris().target

np.random.seed(116)
np.random.shuffle(x_train)
np.random.seed(116)
np.random.shuffle(y_train)
tf.random.set_seed(116)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(3, activation=&apos;softmax&apos;, kernel_regularizer=tf.keras.regularizers.l2())
])

model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=[&apos;sparse_categorical_accuracy&apos;])

model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)

model.summary()</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/27/%E9%B8%A2%E9%B8%9F%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/%E9%B8%A2%E9%B8%9F%E9%9B%86/" class="post-title-link" itemprop="url">鸢鸟集</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-27 20:52:42" itemprop="dateCreated datePublished" datetime="2021-01-27T20:52:42+08:00">2021-01-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-30 21:26:29" itemprop="dateModified" datetime="2021-01-30T21:26:29+08:00">2021-01-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="🎏实现鸢尾花分类整体思路"><a href="#🎏实现鸢尾花分类整体思路" class="headerlink" title="🎏实现鸢尾花分类整体思路"></a>🎏实现鸢尾花分类整体思路</h1><h2 id="🎋准备数据"><a href="#🎋准备数据" class="headerlink" title="🎋准备数据"></a>🎋准备数据</h2><ul>
<li>数据集读入</li>
<li>数据集乱序</li>
<li>生成训练集和测试集</li>
<li>配成（输入特征、标签）对，每次读入batch    </li>
</ul>
<h2 id="🎋搭建网络"><a href="#🎋搭建网络" class="headerlink" title="🎋搭建网络"></a>🎋搭建网络</h2><ul>
<li>定义神经网络中所有可训练参数    </li>
</ul>
<h2 id="🎋参数优化"><a href="#🎋参数优化" class="headerlink" title="🎋参数优化"></a>🎋参数优化</h2><ul>
<li>嵌套循环迭代，with结构更新参数，显示当前loss   </li>
</ul>
<h2 id="🎋测试效果"><a href="#🎋测试效果" class="headerlink" title="🎋测试效果"></a>🎋测试效果</h2><ul>
<li>计算当前参数前后传播后的准确率，显示当前acc  </li>
</ul>
<h2 id="🎋acc-loss可视化"><a href="#🎋acc-loss可视化" class="headerlink" title="🎋acc/loss可视化"></a>🎋acc/loss可视化</h2><h1 id="🎏准备"><a href="#🎏准备" class="headerlink" title="🎏准备"></a>🎏准备</h1><h2 id="🎋数据集读入"><a href="#🎋数据集读入" class="headerlink" title="🎋数据集读入"></a>🎋数据集读入</h2><p>从<code>sklearn.datasets</code>中可以得到鸢鸟的数据包<br>也就是，首先要有<code>sklearn</code>包,通过<code>pip instal sklearn</code>获得</p>
<pre><code>from sklearn.datasets import load_iris
from pandas import DataFrame  #处理数据，形成表格，更直观
import pandas as pd

x_data = load_iris().data
y_data = load_iris().target
#获得特征和对应的标签

x_data = DataFrame(x_data, columns=[&apos;花萼长度&apos;, &apos;花萼宽度&apos;, &apos;花瓣长度&apos;, &apos;花瓣宽度&apos;]) # 为表格增加行索引（左侧）和列标签（上方）
pd.set_option(&apos;display.unicode.east_asian_width&apos;, True)  # 设置列名对齐

x_data[&apos;类别&apos;] = y_data  # 新加一列，列标签为‘类别’，数据为y_data

print(x_data)</code></pre><p><img src="https://i.loli.net/2021/01/27/u1WnPpVtriXq9Gj.png" alt="结果"></p>
<h2 id="🎱关于DataFrame"><a href="#🎱关于DataFrame" class="headerlink" title="🎱关于DataFrame"></a>🎱关于DataFrame</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>创建一个DataFrame，相当于这一堆数据就是一个表格，创建时，会默认给新增一列在最左侧，是索引，从0开始；<br>如果数据是字典型，则已经确定好了列名。<br>若是数组，则可通过<code>columns=[]</code>来命名各列。<br><img src="https://i.loli.net/2021/01/27/LOs6ztbnkuYgAPx.png" alt="例子"></p>
<h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>创建之后，就是一个可操作的DataFrame<br>则若要新增一列(最后一列)，可直接<code>x[&#39;列名&#39;]=[内容]</code>。<br>若要指定插入一列<code>x.insert(插入后所处的列数，列名，内容)</code><br><img src="https://i.loli.net/2021/01/27/AJx4Sdse9h2w1VU.png" alt="image.png"></p>
<h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><p><strong>列</strong><br>可直接<code>x[列名]</code><br>当然可直接通过此赋值<code>x[列名]=[内容]</code><br><strong>行</strong><br>直接<code>x.loc[&#39;行名&#39;]</code>   </p>
<p><strong>行和列</strong>  </p>
<pre><code>x.iloc[:,:]   
#可以看到，第一个是几行，第二个是几列。
#注意，都是从0计数且前闭后开 </code></pre><p><img src="https://i.loli.net/2021/01/27/InPzHicRCvDZMyS.png" alt="image.png"></p>
<h3 id="索引补充"><a href="#索引补充" class="headerlink" title="索引补充"></a>索引补充</h3><p>索引除了最开始在创建时创建以外，在创建后也可以更改。   </p>
<ol>
<li><p>将自己的一列设置为索引<code>x.set_index([列名],drop=True)</code><br>drop默认是True，意思是，将该列作为索引后，不保存该列数据；改为False，则保存该列数据。<br><img src="https://i.loli.net/2021/01/27/jksDGTqL9dFHhWa.png" alt="image.png"></p>
</li>
<li><p>直接创建<code>x.index=[1,2,3,4,5]</code></p>
</li>
<li><p>还原成默认值<code>x.reset_index(drop=False)</code><br>drop默认值是False，是指将原来的索引值保留为数据；若为True，则原索引值删除。<br><img src="https://i.loli.net/2021/01/27/TbaqnWCr7gHPOLl.png" alt="image.png"></p>
</li>
</ol>
<h2 id="🎋数据集乱序"><a href="#🎋数据集乱序" class="headerlink" title="🎋数据集乱序"></a>🎋数据集乱序</h2><pre><code>np.random.seed(116) #是用相同的seed，使输入特征/标签一一对应
np.random.shuffle(x_data)
np.random.seed(116)
np.random.shuffle(y_data)
tf.random.set_seed(116)</code></pre><h2 id="🎋分为训练集和测试集"><a href="#🎋分为训练集和测试集" class="headerlink" title="🎋分为训练集和测试集"></a>🎋分为训练集和测试集</h2><pre><code>x_train = x_data[:-30]
y_train = y_data[:-30]
x_test = x_data[-30:]
y_test = y_data[-30:]</code></pre><h2 id="🎋配对"><a href="#🎋配对" class="headerlink" title="🎋配对"></a>🎋配对</h2><pre><code>train_db = tf.data.Dataset.from_tensor_slices((x_trainm4,y_train)).batch(32)
test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)</code></pre><h1 id="🎏搭建网络"><a href="#🎏搭建网络" class="headerlink" title="🎏搭建网络"></a>🎏搭建网络</h1><pre><code>w1 = tf.Variable(tf.random.truncated_normal([4,3],stddev=0.1,seed=1))
b1 = tf.Variable(tf.random.truncated_normal([4,3],stddev=0.1,seed=1))

for epoch in range(epoch):
    for step,(x_train,y_train) in enumerate(train_db):
        with tf.GradientTape() as tape:
            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算
            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）
            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy
            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)
            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确

            grads = tape.gradient(loss,[w1,b1])
            w.assign_sub(lr*grads[0])
            b1.assign_sub(lr*grads[1])
        print(&quot;Epoch{},loss:{}&quot;.format(epoch,loss_all/4))</code></pre><h1 id="🎏计算准确率"><a href="#🎏计算准确率" class="headerlink" title="🎏计算准确率"></a>🎏计算准确率</h1><pre><code>for x_test,y_test in test_db:
    y = tf.matul(h,w)+b  #预测结果
    y = tf.nn.softmax(y)  #y符合概率分布  
    pred = tf.argmax(y,axis=1) #返回y中最大值的索引，即预测的分类  
    pred = tf.cast(pred,dtype = y_test.dtype) #调整数据类型与标签一致  
    correct = tf.cast(tf.equal(pred,y_test),dtype = tf.int32)
    correct = tf.reduce_sum(correct) $将每个batch的correct数加起来
    total_correct +=int(correct)  #将所有batch中的correct数加起来  
    total_number += x_test.shape[0]
acc = total_correct/total_number
print(&quot;test_acc:&quot;,acc)

plt.title(&quot;Acc Curve&quot;)
plt.xlabel(&quot;Epoch&quot;)
plt.ylabel(&quot;Acc&quot;)
plt.plot(test_acc,label=&quot;$Accuracy$&quot;)  
plt.legend()
plt.show()</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/24/Tensorflow-Anaconda-Pycharm%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/24/Tensorflow-Anaconda-Pycharm%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/" class="post-title-link" itemprop="url">Tensorflow+Anaconda+Pycharm环境安装</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-24 15:39:12" itemprop="dateCreated datePublished" datetime="2021-01-24T15:39:12+08:00">2021-01-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.1.1
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
